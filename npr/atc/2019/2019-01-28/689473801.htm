<h1> "YouTube To Stop Promoting Videos That Spread Misinformation" </h1>
    <p>AUDIE CORNISH, HOST: </p>
    <p>Watch a video on YouTube, and you'll see a list of recommendations for what to watch next based on what you're watching at the moment and your search history. But it doesn't take much to go from a video that's fairly innocuous to one that promotes conspiracy theories.</p>
    <p>That happens frequently enough that YouTube has come under pressure to change its algorithm. It says it will now promote fewer videos of what it calls borderline content. NPR's Andrew Limbong has more.</p>
    <p>ANDREW LIMBONG, BYLINE: In a blog post, YouTube defines borderline content as things that, quote, "misinform users in harmful ways" but don't quite violate their community guidelines. The company specifically cites flat Earth conspiracies...</p>
    <p>(SOUNDBITE OF ARCHIVED RECORDING)</p>
    <p>UNIDENTIFIED PERSON #1: We do not believe that we're flying in space whatsoever. We don't believe the Earth moves at all.</p>
    <p>LIMBONG: ...Phony miracle cures and 9/11 truth videos.</p>
    <p>(SOUNDBITE OF ARCHIVED RECORDING)</p>
    <p>UNIDENTIFIED PERSON #2: Western civilization is doomed unless we face the unanswered questions of 9/11.</p>
    <p>LIMBONG: There are plenty of other misinformation videos on YouTube, from anti-vaccine rants to conspiracies of school shootings being faked.</p>
    <p>(SOUNDBITE OF ARCHIVED RECORDING)</p>
    <p>JOHN BOUCHELL: In my utterly qualified, expert opinion, there are several troubling facts being dispensed that I refuse to accept.</p>
    <p>LIMBONG: These misinformation and conspiracy videos will still all exist on YouTube. They just won't be recommended to you. You'll have to look for them.</p>
    <p>Google, which owns YouTube, declined to offer anyone up for an interview, but the company says it will, quote, "work with human evaluators and experts from all over the United States to help train the machine learning systems that generate recommendations."</p>
    <p>Zeynep Tufekci is an associate professor at the University of North Carolina studying the social impacts of digital technology and artificial intelligence. She says the big problem with the YouTube recommendation machine is that it's designed to get you to spend as much time on the platform as possible so they can sell more ads. The accuracy of the content doesn't matter.</p>
    <p>ZEYNEP TUFEKCI: Just like a cafeteria, you're going to get people to eat more if you serve unhealthy food again and again and again before they even have a chance to finish their plate.</p>
    <p>LIMBONG: And its effect - she adds that it's increasingly schoolchildren turning to YouTube for information and getting fed these types of videos. She wrote about the issue a while back in The New York Times.</p>
    <p>TUFEKCI: And I got flooded with examples and comments. Like, parents would put their kid in front of YouTube with a video from NASA - right? - some very innocuous, interesting content, which YouTube is full of. And 45 minutes later, the kid would come back and say, Mom, the moon landing never happened.</p>
    <p>LIMBONG: YouTube is rolling out these changes to its recommendation machine gradually in the United States first, affecting less than 1 percent of all YouTube content. But Tufekci says it's around the rest of the world - Brazil, Indonesia, Sri Lanka - where misinformation on YouTube truly has the power to destabilize societies.</p>
    <p>Andrew Limbong, NPR News.</p>
